{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9312eb8-225a-42a8-940c-8fb3d06a6b16",
   "metadata": {},
   "source": [
    "TODO: add summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5ed77d-58e5-475b-b48b-ff84bb139771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import gpt_2_simple as gpt2\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6e1871-94f6-4d98-a814-f5d946d9005b",
   "metadata": {},
   "source": [
    "TODO: add summary -- get the tweets and place in text file\n",
    "TODO: could clean up tweets a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbf2d124-2bc4-441c-ace3-4f238cc664d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the tweets dataset\n",
    "tweets = pd.read_json('..\\\\..\\\\..\\\\datasets\\\\vol3\\\\somethingAboutRiddlesAndFruit.json',\n",
    "                      lines = True)\n",
    "\n",
    "#Create directory to store data in (if needed)\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "#Open up a text file to store tweets\n",
    "with open('data\\\\tweetsText.txt', 'w') as f:\n",
    "    \n",
    "    #Loop through tweets to write\n",
    "    for currTweet in tweets['content']:\n",
    "        \n",
    "        #Strip lines from current tweet\n",
    "        strippedTweet = currTweet.replace('\\n',' ')\n",
    "        \n",
    "        #Encode tweet to ascii\n",
    "        asciiTweet = strippedTweet.encode(\"ascii\", \"ignore\")\n",
    "        \n",
    "        #Write current tweet to file\n",
    "        f.write(asciiTweet.decode())\n",
    "        \n",
    "        #Write the tweet separator\n",
    "        f.write('\\n==========\\n')\n",
    "\n",
    "#Close the text file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87629116-8357-4b0c-8bdd-9e837bf8b834",
   "metadata": {},
   "source": [
    "TODO: notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3edbd5e-2446-4978-b9c1-0a756a1846b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 150Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:03, 303kit/s]                                                    \n",
      "Fetching hparams.json: 1.05Mit [00:00, 150Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [1:01:35, 135kit/s]                                 \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 102Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:02, 384kit/s]                                                 \n",
      "Fetching vocab.bpe: 1.05Mit [00:03, 322kit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "#Get the pretrained model from the gpt2 library\n",
    "modelName = \"124M\"\n",
    "\n",
    "#Download the model if it is not present\n",
    "if not os.path.isdir(os.path.join(\"models\", modelName)):\n",
    "    gpt2.download_gpt2(model_name = modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d46d7d-bf88-43ec-8120-524e909ea8e0",
   "metadata": {},
   "source": [
    "The default number of words passed to the model in a step is set with the sample_length parameter and is 1023 by default. So the number of words divided by 1023 will dictate how many steps it takes to go through the data one time. The number of steps needs to be high enough to go through the data at least once, but not too many times (i.e. once or twice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542a6cba-2534-4f49-abe3-77e27f354d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps to work through entire dataset: 33.505376344086024\n"
     ]
    }
   ],
   "source": [
    "#Number of words\n",
    "file = open('data\\\\tweetsText.txt', 'rt')\n",
    "data = file.read()\n",
    "words = data.split()\n",
    "\n",
    "#Set the number of words per sample length\n",
    "sampleLength = 1023\n",
    "\n",
    "#Print number of steps taken to get through dataset\n",
    "print('Number of steps to work through entire dataset:', len(words) / sampleLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6a0af7-2d69-4ff2-b53d-e50b9543aff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint models\\124M\\model.ckpt\n",
      "INFO:tensorflow:Restoring parameters from models\\124M\\model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 78542 tokens\n",
      "Training...\n",
      "[1 | 69.92] loss=3.31 avg=3.31\n",
      "[2 | 150.86] loss=3.56 avg=3.44\n",
      "[3 | 232.43] loss=3.18 avg=3.35\n",
      "[4 | 312.27] loss=3.17 avg=3.31\n",
      "[5 | 388.77] loss=3.28 avg=3.30\n",
      "[6 | 465.47] loss=3.40 avg=3.32\n",
      "[7 | 541.24] loss=2.94 avg=3.26\n",
      "[8 | 619.67] loss=3.16 avg=3.25\n",
      "[9 | 702.52] loss=3.13 avg=3.24\n",
      "[10 | 781.40] loss=3.07 avg=3.22\n",
      "[11 | 862.33] loss=2.88 avg=3.19\n",
      "[12 | 938.71] loss=2.88 avg=3.16\n",
      "[13 | 1014.26] loss=2.88 avg=3.14\n",
      "[14 | 1089.94] loss=2.68 avg=3.10\n",
      "[15 | 1165.97] loss=2.89 avg=3.09\n",
      "[16 | 1242.36] loss=2.96 avg=3.08\n",
      "[17 | 1320.96] loss=2.79 avg=3.06\n",
      "[18 | 1402.85] loss=2.81 avg=3.04\n",
      "[19 | 1482.18] loss=2.70 avg=3.02\n",
      "[20 | 1557.14] loss=2.65 avg=3.00\n",
      "[21 | 1633.31] loss=2.52 avg=2.98\n",
      "[22 | 1708.82] loss=2.73 avg=2.97\n",
      "[23 | 1783.60] loss=2.72 avg=2.95\n",
      "[24 | 1858.82] loss=2.72 avg=2.94\n",
      "[25 | 1935.68] loss=2.70 avg=2.93\n",
      "[26 | 2014.47] loss=2.57 avg=2.92\n",
      "[27 | 2090.38] loss=2.53 avg=2.90\n",
      "[28 | 2166.02] loss=2.43 avg=2.88\n",
      "[29 | 2241.46] loss=2.42 avg=2.86\n",
      "[30 | 2318.38] loss=2.36 avg=2.84\n",
      "[31 | 2393.67] loss=2.67 avg=2.84\n",
      "[32 | 2469.34] loss=2.46 avg=2.82\n",
      "[33 | 2546.95] loss=2.30 avg=2.81\n",
      "[34 | 2626.97] loss=2.30 avg=2.79\n",
      "[35 | 2704.14] loss=2.65 avg=2.78\n",
      "[36 | 2779.50] loss=2.39 avg=2.77\n",
      "[37 | 2854.78] loss=2.60 avg=2.76\n",
      "[38 | 2933.11] loss=2.50 avg=2.76\n",
      "[39 | 3019.69] loss=2.36 avg=2.74\n",
      "[40 | 3104.90] loss=2.27 avg=2.73\n",
      "[41 | 3192.75] loss=2.41 avg=2.72\n",
      "[42 | 3281.11] loss=2.26 avg=2.71\n",
      "[43 | 3369.42] loss=2.45 avg=2.70\n",
      "[44 | 3459.89] loss=2.14 avg=2.68\n",
      "[45 | 3547.85] loss=2.45 avg=2.68\n",
      "[46 | 3635.41] loss=2.27 avg=2.67\n",
      "[47 | 3722.51] loss=2.51 avg=2.66\n",
      "[48 | 3809.53] loss=2.08 avg=2.65\n",
      "[49 | 3894.72] loss=2.26 avg=2.64\n",
      "[50 | 3978.06] loss=2.46 avg=2.63\n",
      "[51 | 4055.21] loss=2.22 avg=2.62\n",
      "[52 | 4130.62] loss=2.35 avg=2.62\n",
      "[53 | 4207.16] loss=2.35 avg=2.61\n",
      "[54 | 4282.44] loss=2.32 avg=2.60\n",
      "[55 | 4360.77] loss=2.28 avg=2.60\n",
      "[56 | 4439.34] loss=2.19 avg=2.59\n",
      "[57 | 4515.40] loss=2.10 avg=2.57\n",
      "[58 | 4590.78] loss=2.41 avg=2.57\n",
      "[59 | 4665.64] loss=2.08 avg=2.56\n",
      "[60 | 4741.50] loss=2.12 avg=2.55\n",
      "[61 | 4817.33] loss=2.15 avg=2.54\n",
      "[62 | 4893.10] loss=2.20 avg=2.53\n",
      "[63 | 4973.21] loss=2.29 avg=2.53\n",
      "[64 | 5053.54] loss=1.96 avg=2.52\n",
      "[65 | 5129.54] loss=1.96 avg=2.51\n",
      "[66 | 5204.89] loss=2.13 avg=2.50\n",
      "[67 | 5288.44] loss=2.13 avg=2.49\n",
      "[68 | 5373.05] loss=2.14 avg=2.48\n",
      "[69 | 5456.42] loss=2.16 avg=2.48\n",
      "[70 | 5532.99] loss=1.96 avg=2.47\n",
      "[71 | 5612.39] loss=1.81 avg=2.45\n",
      "[72 | 5690.54] loss=1.85 avg=2.44\n",
      "[73 | 5766.62] loss=1.84 avg=2.43\n",
      "[74 | 5844.02] loss=2.05 avg=2.42\n",
      "[75 | 5921.39] loss=1.98 avg=2.41\n",
      "Saving checkpoint\\run1\\model-75\n",
      "SIGNING ANNOUNCEMENTS:   Simon Leonard has re-signed with the @MelbourneVixens for a further two seasons.  Full details - https://t.co/6dRpPe0bmq  #SSNTrade #SSNSignings #netball https://t.co/3YG4p1XKyz\n",
      "==========\n",
      "The @MelbourneVixens have announced that the versatile defender has re-signed for the next two seasons.    Simon Leonard has also re-signed with @MelbourneVixens for a further two seasons!   Read more about the move here: https://t.co/LxSZDw3LbP  #SSNTrade #SSNSignings https://t.co/TzKpz3D5dE\n",
      "==========\n",
      "Well, I feel like the @MelbourneVixens signing period is over.  They have another player to go around and another defender to go around, I think this is going to be a fun time.  @AdelaideTBirds signing samantha firebirds  #SSNTrade #SSNSignings https://t.co/7zQjKJY0D9\n",
      "==========\n",
      "All the latest @AdelaideTBirds news here: https://t.co/sThePc1PuR https://t.co/4yTc0x2Tqn\n",
      "==========\n",
      "Well I think the @MelbourneVixens signing period is done and the @FirebirdsQld have announced that Simon Leonard has signed on as a full-time defender for 2022. #SSNTrade #SSNSignings   Story by @jessicamiles01  https://t.co/BUt0Ug7lFZb\n",
      "==========\n",
      "Interesting #SSNTrade news today  https://t.co/4tBJ2D0feX\n",
      "==========\n",
      "I have a feeling Phumza is back in purple next year. Who will it be? #SSNTrade #SSNSignings\n",
      "==========\n",
      "The @MelbourneVixens have re-signed defender Simon Leonard for the next two seasons.   Simon has been with the club for two seasons and #SSNtrade is over for both.   #SSNsignings https://t.co/LnPwKg2prn\n",
      "==========\n",
      "So after signing my SSN signing card and confused at first. Now I know what I'm signing and I can keep it! #SSNTrade\n",
      "==========\n",
      "Wow, wasn't expecting this one. This card is the start of a new level for me!  Ive had amazing fortune telling me things I never even knew I could even imagine  #SSNTrade #SSNSignings\n",
      "==========\n",
      "INBOX: Netball clubs have elevated Samantha Firebirds defender Simon Leonard to the final roster for 2022. #SSNTrade #SSNSignings\n",
      "==========\n",
      "The @AdelaideTBirds have elevated captain Simon Leonard to the final roster for 2022.   Simon has been elevated to the final position for all three sessions of the @SuperNetball training camp.    Simon has also been elevated to the sideline.   Simon Leonard is in the final year of his contract with the @AdelaideTBirds.  #SSNTrade   https://t.co/3yVZkqIwQm\n",
      "==========\n",
      "PLAYER SIGNING   Simon Leonard has re-signed with the @AdelaideTBirds for the 2022 season.  Full story &gt;&gt; https://t.co/n9CjVcE2zO   Simon Leonard #SSNTrade https://t.co/FyGcE0B2Jd\n",
      "==========\n",
      "INBOX: @AdelaideTBirds captain Simon Leonard has re-signed with the club for the 2022 season.  Full story &gt; https://t.co/OpE0UjdjJm  #SSNTrade #SSNSignings https://t.co/3yVztoZydF\n",
      "==========\n",
      "Magpies announce big #SSNTrade signing of the #SSNSignings period.   Leonard has re-signed with the club for the next two seasons.  #SSNtrade #SSNSignings https://t.co/lWsz5j6V3Io\n",
      "==========\n",
      "BREAKING: @SuperNetball has elevated captain Simon Leonard to the final team for the 2022 @SuperNetball training camp.  Full story &gt; https://t.co/YPFv1aL8oA  #SSNSignings #SSNTrade https://t.co/qI9PXf3\n"
     ]
    }
   ],
   "source": [
    "#Set model name\n",
    "modelName = \"124M\"\n",
    "\n",
    "#Start a Tensorflow session to pass to gpt2_simple\n",
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "# Define the number of steps we want our model to take we want this to be such that\n",
    "# we only pass over the data set 1-2 times to avoid overfitting.\n",
    "numSteps = 75\n",
    "\n",
    "# This is the path to the text file we want to use for training.\n",
    "textPath = 'data\\\\tweetsText.txt'\n",
    "\n",
    "# Pass in the session and the\n",
    "gpt2.finetune(sess,\n",
    "              textPath,\n",
    "              model_name = modelName,\n",
    "              steps = numSteps\n",
    "             )\n",
    "\n",
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27715df3-2887-4b9f-a22b-47d751931e47",
   "metadata": {},
   "source": [
    "TODO: summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe65981e-9bb7-4cb3-a07f-16fbbbf475c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint\\run1\\model-75\n",
      "INFO:tensorflow:Restoring parameters from checkpoint\\run1\\model-75\n"
     ]
    }
   ],
   "source": [
    "#Need to restart kernel before doing this - I think so?\n",
    "\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, checkpoint_dir = 'checkpoint\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36814624-9d99-4ee7-aa0e-b6121a9fb90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Trade news: @MelbourneVixens captain O Nisha looks set to make the move to the Vixens.   #SSNTrade #SSNSignings https://t.co/DqvLpXOO4V\\n==========\\n@dcoppel @CoachKellyYourentGee who is it going to be? #SSNTrade #SSNSignings https://t.co/vqizxCz80D\\n==========\\nThe @MelbourneVixens have decided to go big by signing former Sunshine Coast shooter O Nisha.    Lauren Moore/INNSNewsmedia   #SSNSignings #SSNTrade https://t.co/yZKpA6Micc\\n==========\\nMelbourne Vixens have announced that former Sunshine Coast captain O Nisha has signed on.    The @MelbourneVixens have confirmed that O Nisha will be back in 2022.    Simon Leonard | Netball Scoop  #SSNSignings #SSNTrade   https://t.co/Xjlp5uOQn0\\n==========\\nThe @MelbourneVixens have added their player to the 2022 roster. The 5-foot-11 international has signed on for 2022. #SSNTrade #SSNSignings   https://t.co/L92DZ9jr7Z\\n==========\\nVerdict: Fever have the crown in Melbourne. The Fever attack has its work cut out for it with plenty of goal attack talent available to the club.    Will O Shred &amp; Sterling complete the Maloney circle?? #SSNTrade #SSNSignings https://t.co/eOQ5wX4Bj7\\n==========\\nPlayer Announcement   @MelbourneVixens front end is back, when they have key attacking midcourter Kiera Verity! #SSNTrade #SSNSignings   @jessicamiles93 @TheNetballShow @EmmaGreenwood12  @CoachKellyYourentGee #SSNTrade https://t.co/k55qwOvE1o\\n==========\\nGo Premiership @MelbourneVixens and the reigning Champions have signed on for the 2022 season. Who will be the last name for the team to close its ranks? #SSNTrade #SSNSignings   @jessicamiles93  @maddyproud @TheNetballShow @EmmaGreenwood12\\n==========\\nCan we go Shooter? #SSNTrade https://t.co/8GkfLp7XsE\\n==========\\nVerity has re-signed with @MelbourneVixens for the 2022 season! #SSNTrade #SSNSignings\\n==========\\nPlayer Announcement  Verity will join the @MelbourneVixens in the 2022 season. #SSNTrade #SSNSignings    Simon Leonard | Netball Scoop   #SSNSignings #SSNTrade https://t.co/4YsjQchk6H\\n==========\\nThe @MelbourneVixens have announced that Kiera Verity has re-signed with the club for the 2022 season. #SSNTrade #SSNSignings   @dcoppel  #NetballKorea #chefsoftelevision\\n==========\\nGuy Voss #SSNTrade  STAYING AT Vixens\\n==========\\nThis is one of those articles, unexpected and fab, that you will want to read.    If you are looking for some background on Diamonds captain, @kiera_verity #ssntrade\\n==========\\nKiera Sinclair #SSNTrade  STAYING AT Vixens\\n==========\\nThe @MelbourneVixens have announced that Kiera Verity has re-signed with the club for the 2022 season! #SSNTrade #SSNSignings   @maddyproud    Simon Leonard | Netball Scoop   #SSNSignings #SSNTrade https://t.co/g1teFCrYhZ\\n==========\\nSo, the @Vixens announced Kiera Verity had re-signed with them for the next two seasons!  Aside from her breakout year, Verity is also widely recognised as one of the sport's best goal attack defenders.   #SSNSignings #SSNTrade https://t.co/G1Gv8U2w8H\\n==========\\nWhat a signing! Verity is a star in the eye of the netball season right now. I can honestly say\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = gpt2.generate(\n",
    "    sess,\n",
    "    length = 1000,\n",
    "    temperature = 0.8, #between 0.8 & 2; lower value = more sane\n",
    "    destination_path = None,\n",
    "    return_as_list=True\n",
    ")\n",
    "\n",
    "#Display\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7069a2d4-3a26-49d2-8bdb-04cc431e42c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable model/wpe/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25204/2722804613.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m gpt2.finetune(sess, reuse = True,\n\u001b[0m\u001b[0;32m      3\u001b[0m           \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtextPath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\gpt_2_simple\\gpt_2.py\u001b[0m in \u001b[0;36mfinetune\u001b[1;34m(sess, dataset, steps, model_name, model_dir, combine, batch_size, learning_rate, accumulate_gradients, restore_from, run_name, checkpoint_dir, sample_every, sample_length, sample_num, multi_gpu, save_every, print_every, max_checkpoints, use_memory_saving_gradients, only_train_transformer_layers, optimizer, overwrite, reuse)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mopt_reset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mopt_compute\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0mopt_apply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[0msummary_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_apply\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\gpt_2_simple\\src\\accumulate.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccum_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    603\u001b[0m                        ([str(v) for _, v, _ in converted_grads_and_vars],))\n\u001b[0;32m    604\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_on_eager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\u001b[0m in \u001b[0;36m_create_slots\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;31m# Create slots for the first and second moments.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"m\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"v\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36m_zeros_slot\u001b[1;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m       new_slot_variable = slot_creator.create_zeros_slot(\n\u001b[0m\u001b[0;32m   1157\u001b[0m           var, op_name, copy_xla_sharding=True)\n\u001b[0;32m   1158\u001b[0m       self._restore_slot_variable(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[1;34m(primary, name, dtype, colocate_with_primary, copy_xla_sharding)\u001b[0m\n\u001b[0;32m    253\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[0minitializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m     return create_slot_with_initializer(\n\u001b[0m\u001b[0;32m    256\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[1;34m(primary, initializer, shape, dtype, name, colocate_with_primary, copy_xla_sharding)\u001b[0m\n\u001b[0;32m    209\u001b[0m       \u001b[0mdistribution_strategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_vars_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         return _create_slot_var(\n\u001b[0m\u001b[0;32m    212\u001b[0m             \u001b[0mprimary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\u001b[0m in \u001b[0;36m_create_slot_var\u001b[1;34m(primary, val, scope, validate_shape, shape, dtype, copy_xla_sharding)\u001b[0m\n\u001b[0;32m     72\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0muse_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m   slot = variable_scope.get_variable(\n\u001b[0m\u001b[0;32m     75\u001b[0m       \u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1577\u001b[0m                  \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVariableSynchronization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m                  aggregation=VariableAggregation.NONE):\n\u001b[1;32m-> 1579\u001b[1;33m   return get_variable_scope().get_variable(\n\u001b[0m\u001b[0;32m   1580\u001b[0m       \u001b[0m_get_default_variable_store\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1320\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       return var_store.get_variable(\n\u001b[0m\u001b[0;32m   1323\u001b[0m           \u001b[0mfull_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    576\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m       return _true_getter(\n\u001b[0m\u001b[0;32m    579\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \"name was already created with partitioning?\" % name)\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m       return self._get_single_variable(\n\u001b[0m\u001b[0;32m    532\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m           \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\snscrape_py38\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    885\u001b[0m         \u001b[1;31m# ResourceVariables don't have an op associated with so no traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResourceVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[1;31m# Throw away internal tf entries and only take a few lines. In some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable model/wpe/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2375c7-13a0-4794-8de5-8ef8162124df",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.load_gpt2(sess, checkpoint_dir = 'checkpoint\\\\', reuse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cc553-3c4a-434c-8679-876c9463f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2.generate(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aa6c02-3efc-4957-8372-8975a43ceb62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
